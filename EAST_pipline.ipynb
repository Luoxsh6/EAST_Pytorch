{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/SakuraRiven/EAST/\n",
    "\n",
    "https://blog.csdn.net/qq_41576083/article/details/88077185#3.2%20generate_rbox%E6%A0%87%E7%AD%BE%E7%94%9F%E6%88%90\n",
    "\n",
    "https://shimo.im/docs/pRDd9hddhw9RDWxH/read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:::img_489.jpg\n",
      "[[[915, 501], [953, 523], [948, 554], [910, 533]], [[870, 491], [1025, 583], [1010, 635], [855, 544]], [[921, 514], [942, 527], [940, 539], [919, 527]], [[921.0, 514.0], [934.0, 493.0], [946.0, 495.0], [934.0, 516.0]], [[921.0, 514.0], [908.0, 535.0], [896.0, 533.0], [908.0, 512.0]]]\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "#python 3\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "root_ = \"/home/luoxiaosheng/Downloads/CV_formal/OCR/ICDAR/ch4/\"\n",
    "gt_path = root_ + \"ch4_training_gt/\"\n",
    "img_path = root_ + \"ch4_training_images/\"\n",
    "root_save = root_ + \"ICDAR_with_GT/\"\n",
    "# os.mkdir(root_save)\n",
    "txt_list = os.listdir(gt_path)\n",
    "\n",
    "cnt_ = 0    #count\n",
    "for file_name in txt_list[:1]:\n",
    "    \n",
    "    cnt_ += 1\n",
    "    img_name = file_name.replace('gt_','')\n",
    "    img_name = img_name.replace('.txt','.jpg')\n",
    "    print(\"%d:::%s\"%(cnt_,img_name))\n",
    "    \n",
    "    path_img = img_path + img_name\n",
    "    img = cv2.imread(path_img)\n",
    "    path_txt = gt_path + file_name\n",
    "    \n",
    "    #generate 4 pair-points\n",
    "    with open(path_txt,'r',encoding='utf-8') as f:\n",
    "        str = f.readlines()\n",
    "    ll_pt = []\n",
    "    for path in str:\n",
    "#         print (path.strip())\n",
    "        path = path.strip()\n",
    "        list_str = path.encode('utf-8').decode('utf-8-sig').split(',')\n",
    "#         print(list_str)\n",
    "        l_pt = []\n",
    "        l_pt_tmp = []\n",
    "        for i in range(0,8):\n",
    "            l_pt.append(int(list_str[i]))\n",
    "            if i % 2 != 0:\n",
    "                l_pt_tmp.append(l_pt)\n",
    "                l_pt = []\n",
    "        ll_pt.append(l_pt_tmp)\n",
    "                 #[[915, 501], [953, 523], [948, 554], [910, 533]]\n",
    "    ll_pt.append([[921, 514], [942, 527], [940, 539], [919, 527]])    #shrink box\n",
    "    ll_pt.append([[921., 514.], [934., 493.], [946., 495.], [934., 516.]])   #rotate vertices -90\n",
    "    ll_pt.append([[921., 514.], [908., 535.], [896., 533.], [908., 512.]])\n",
    "    print(ll_pt)\n",
    "    #draw\n",
    "    for v_pt in ll_pt:\n",
    "#         print(v_pt)\n",
    "        point = np.array(v_pt,np.int32)\n",
    "        cv2.polylines(img, [point], True, (0, 255, 255))\n",
    "\n",
    "    # cv2.imshow('helo',img)\n",
    "    # cv2.waitKey(0)\n",
    "    draw_img_path = root_save + img_name\n",
    "    cv2.imwrite(draw_img_path,img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_distance(x1, y1, x2, y2):\n",
    "    '''calculate the Euclidean distance'''\n",
    "    return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "def move_points(vertices, index1, index2, r, coef):\n",
    "    '''move the two points to shrink edge\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        index1  : offset of point1\n",
    "        index2  : offset of point2\n",
    "        r       : [r1, r2, r3, r4] in paper\n",
    "        coef    : shrink ratio in paper\n",
    "    Output:\n",
    "        vertices: vertices where one edge has been shinked\n",
    "    '''\n",
    "    index1 = index1 % 4\n",
    "    index2 = index2 % 4\n",
    "    x1_index = index1 * 2 + 0\n",
    "    y1_index = index1 * 2 + 1\n",
    "    x2_index = index2 * 2 + 0\n",
    "    y2_index = index2 * 2 + 1\n",
    "\n",
    "    r1 = r[index1]\n",
    "    r2 = r[index2]\n",
    "    length_x = vertices[x1_index] - vertices[x2_index]\n",
    "    length_y = vertices[y1_index] - vertices[y2_index]\n",
    "    length = cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index])\n",
    "    if length > 1:\n",
    "        ratio = (r1 * coef) / length\n",
    "        vertices[x1_index] += ratio * (-length_x) \n",
    "        vertices[y1_index] += ratio * (-length_y) \n",
    "        ratio = (r2 * coef) / length\n",
    "        vertices[x2_index] += ratio * length_x \n",
    "        vertices[y2_index] += ratio * length_y\n",
    "    return vertices\n",
    "\n",
    "def shrink_poly(vertices, coef=0.3):\n",
    "    '''shrink the text region\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        coef    : shrink ratio in paper\n",
    "    Output:\n",
    "        v       : vertices of shrinked text region <numpy.ndarray, (8,)>\n",
    "    '''\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    r1 = min(cal_distance(x1,y1,x2,y2), cal_distance(x1,y1,x4,y4))\n",
    "    r2 = min(cal_distance(x2,y2,x1,y1), cal_distance(x2,y2,x3,y3))\n",
    "    r3 = min(cal_distance(x3,y3,x2,y2), cal_distance(x3,y3,x4,y4))\n",
    "    r4 = min(cal_distance(x4,y4,x1,y1), cal_distance(x4,y4,x3,y3))\n",
    "    r = [r1, r2, r3, r4]\n",
    "\n",
    "    # obtain offset to perform move_points() automatically\n",
    "    if cal_distance(x1,y1,x2,y2) + cal_distance(x3,y3,x4,y4) > \\\n",
    "       cal_distance(x2,y2,x3,y3) + cal_distance(x1,y1,x4,y4):\n",
    "        offset = 0 # two longer edges are (x1y1-x2y2) & (x3y3-x4y4)\n",
    "    else:\n",
    "        offset = 1 # two longer edges are (x2y2-x3y3) & (x4y4-x1y1)\n",
    "\n",
    "    v = vertices.copy()\n",
    "    v = move_points(v, 0 + offset, 1 + offset, r, coef)\n",
    "    v = move_points(v, 2 + offset, 3 + offset, r, coef)\n",
    "    v = move_points(v, 1 + offset, 2 + offset, r, coef)\n",
    "    v = move_points(v, 3 + offset, 4 + offset, r, coef)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundary(vertices):\n",
    "    '''get the tight boundary around given vertices\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        the boundary\n",
    "    '''\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    x_min = min(x1, x2, x3, x4)\n",
    "    x_max = max(x1, x2, x3, x4)\n",
    "    y_min = min(y1, y2, y3, y4)\n",
    "    y_max = max(y1, y2, y3, y4)\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def cal_error(vertices):\n",
    "    '''default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\n",
    "    calculate the difference between the vertices orientation and default orientation\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        err     : difference measure\n",
    "    '''\n",
    "    x_min, x_max, y_min, y_max = get_boundary(vertices)\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    err = cal_distance(x1, y1, x_min, y_min) + cal_distance(x2, y2, x_max, y_min) + \\\n",
    "          cal_distance(x3, y3, x_max, y_max) + cal_distance(x4, y4, x_min, y_max)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotate_mat(theta):\n",
    "    '''positive theta value means rotate clockwise'''\n",
    "    return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n",
    "\n",
    "\n",
    "def rotate_vertices(vertices, theta, anchor=None):\n",
    "    '''rotate vertices around anchor\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "        theta   : angle in radian measure\n",
    "        anchor  : fixed position during rotation\n",
    "    Output:\n",
    "        rotated vertices <numpy.ndarray, (8,)>\n",
    "    '''\n",
    "    v = vertices.reshape((4,2)).T\n",
    "    if anchor is None:\n",
    "        anchor = v[:,:1]    #pin the first point\n",
    "    rotate_mat = get_rotate_mat(theta)\n",
    "    res = np.dot(rotate_mat, v - anchor)\n",
    "    return (res + anchor).T.reshape(-1)\n",
    "\n",
    "\n",
    "def rotate_img(img, vertices, angle_range=10):\n",
    "    '''rotate image [-10, 10] degree to aug data\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        angle_range : rotate range\n",
    "    Output:\n",
    "        img         : rotated PIL Image\n",
    "        new_vertices: rotated vertices\n",
    "    '''\n",
    "    center_x = (img.width - 1) / 2\n",
    "    center_y = (img.height - 1) / 2\n",
    "    angle = angle_range * (np.random.rand() * 2 - 1)\n",
    "    img = img.rotate(angle, Image.BILINEAR)\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        new_vertices[i,:] = rotate_vertices(vertice, -angle / 180 * math.pi, np.array([[center_x],[center_y]]))\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length):\n",
    "    '''get rotated locations of all pixels for next stages\n",
    "    Input:\n",
    "        rotate_mat: rotatation matrix\n",
    "        anchor_x  : fixed x position\n",
    "        anchor_y  : fixed y position\n",
    "        length    : length of image\n",
    "    Output:\n",
    "        rotated_x : rotated x positions <numpy.ndarray, (length,length)>\n",
    "        rotated_y : rotated y positions <numpy.ndarray, (length,length)>\n",
    "    '''\n",
    "    x = np.arange(length)\n",
    "    y = np.arange(length)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    x_lin = x.reshape((1, x.size))\n",
    "    y_lin = y.reshape((1, x.size))\n",
    "    coord_mat = np.concatenate((x_lin, y_lin), 0)\n",
    "    rotated_coord = np.dot(rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])) + \\\n",
    "                                                   np.array([[anchor_x], [anchor_y]])\n",
    "    rotated_x = rotated_coord[0, :].reshape(x.shape)\n",
    "    rotated_y = rotated_coord[1, :].reshape(y.shape)\n",
    "    return rotated_x, rotated_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_rect_angle(vertices):\n",
    "    '''find the best angle to rotate poly and obtain min rectangle\n",
    "    Input:\n",
    "        vertices: vertices of text region <numpy.ndarray, (8,)>\n",
    "    Output:\n",
    "        the best angle <radian measure>\n",
    "    '''\n",
    "    angle_interval = 1\n",
    "    angle_list = list(range(-90, 90, angle_interval))\n",
    "    area_list = []\n",
    "    for theta in angle_list: \n",
    "        rotated = rotate_vertices(vertices, theta / 180 * math.pi)    #角度=180°×弧度÷π, 弧度=角度×π÷180°\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = rotated\n",
    "        temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * \\\n",
    "                    (max(y1, y2, y3, y4) - min(y1, y2, y3, y4))\n",
    "        area_list.append(temp_area)\n",
    "\n",
    "    sorted_area_index = sorted(list(range(len(area_list))), key=lambda k : area_list[k])    #升序\n",
    "\n",
    "    min_error = float('inf')\n",
    "    best_index = -1\n",
    "    rank_num = 10\n",
    "    # find the best angle with correct orientation\n",
    "    for index in sorted_area_index[:rank_num]:\n",
    "        rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\n",
    "        temp_error = cal_error(rotated)\n",
    "        if temp_error < min_error:\n",
    "            min_error = temp_error\n",
    "            best_index = index\n",
    "    return angle_list[best_index] / 180 * math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cross_text(start_loc, length, vertices):\n",
    "    '''check if the crop image crosses text regions\n",
    "    Input:\n",
    "        start_loc: left-top position\n",
    "        length   : length of crop image\n",
    "        vertices : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "    Output:\n",
    "        True if crop image crosses text region\n",
    "    '''\n",
    "    if vertices.size == 0:\n",
    "        return False\n",
    "    start_w, start_h = start_loc\n",
    "    a = np.array([start_w, start_h, start_w + length, start_h, \\\n",
    "          start_w + length, start_h + length, start_w, start_h + length]).reshape((4,2))\n",
    "    p1 = Polygon(a).convex_hull\n",
    "    for vertice in vertices:\n",
    "        p2 = Polygon(vertice.reshape((4,2))).convex_hull\n",
    "        inter = p1.intersection(p2).area\n",
    "        if 0.01 <= inter / p2.area <= 0.99: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def crop_img(img, vertices, labels, length):\n",
    "    '''crop img patches to obtain batch and augment\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels      : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "        length      : length of cropped image region\n",
    "    Output:\n",
    "        region      : cropped image region\n",
    "        new_vertices: new vertices in cropped region\n",
    "    '''\n",
    "    h, w = img.height, img.width\n",
    "    # confirm the shortest side of image >= length\n",
    "    if h >= w and w < length:\n",
    "        img = img.resize((length, int(h * length / w)), Image.BILINEAR)\n",
    "    elif h < w and h < length:\n",
    "        img = img.resize((int(w * length / h), length), Image.BILINEAR)\n",
    "    ratio_w = img.width / w\n",
    "    ratio_h = img.height / h\n",
    "    assert(ratio_w >= 1 and ratio_h >= 1)\n",
    "\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[0,2,4,6]] = vertices[:,[0,2,4,6]] * ratio_w\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * ratio_h\n",
    "\n",
    "    # find random position\n",
    "    remain_h = img.height - length\n",
    "    remain_w = img.width - length\n",
    "    flag = True\n",
    "    cnt = 0\n",
    "    while flag and cnt < 1000:\n",
    "        cnt += 1\n",
    "        start_w = int(np.random.rand() * remain_w)\n",
    "        start_h = int(np.random.rand() * remain_h)\n",
    "        flag = is_cross_text([start_w, start_h], length, new_vertices[labels==1,:])\n",
    "    box = (start_w, start_h, start_w + length, start_h + length)\n",
    "    region = img.crop(box)\n",
    "    if new_vertices.size == 0:\n",
    "        return region, new_vertices\n",
    "\n",
    "    new_vertices[:,[0,2,4,6]] -= start_w\n",
    "    new_vertices[:,[1,3,5,7]] -= start_h\n",
    "    return region, new_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vertices(lines):\n",
    "    '''extract vertices info from txt lines\n",
    "    Input:\n",
    "        lines   : list of string info\n",
    "    Output:\n",
    "        vertices: vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels  : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "    '''\n",
    "    labels = []\n",
    "    vertices = []\n",
    "    for line in lines:\n",
    "        vertices.append(list(map(int,line.rstrip('\\n').lstrip('\\ufeff').split(',')[:8])))\n",
    "        label = 0 if '###' in line else 1\n",
    "        labels.append(label)\n",
    "    return np.array(vertices), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_height(img, vertices, ratio=0.2):\n",
    "    '''adjust height of image to aug data\n",
    "    Input:\n",
    "        img         : PIL Image\n",
    "        vertices    : vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        ratio       : height changes in [0.8, 1.2]\n",
    "    Output:\n",
    "        img         : adjusted PIL Image\n",
    "        new_vertices: adjusted vertices\n",
    "    '''\n",
    "    ratio_h = 1 + ratio * (np.random.rand() * 2 - 1)\n",
    "    old_h = img.height\n",
    "    new_h = int(np.around(old_h * ratio_h))\n",
    "    img = img.resize((img.width, new_h), Image.BILINEAR)\n",
    "\n",
    "    new_vertices = vertices.copy()\n",
    "    if vertices.size > 0:\n",
    "        new_vertices[:,[1,3,5,7]] = vertices[:,[1,3,5,7]] * (new_h / old_h)\n",
    "    return img, new_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_geo(img, vertices, labels, scale, length):\n",
    "    '''generate score gt and geometry gt\n",
    "    Input:\n",
    "        img     : PIL Image\n",
    "        vertices: vertices of text regions <numpy.ndarray, (n,8)>\n",
    "        labels  : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\n",
    "        scale   : feature map / image\n",
    "        length  : image length\n",
    "    Output:\n",
    "        score gt, geo gt, ignored\n",
    "    '''\n",
    "    score_map   = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n",
    "    geo_map     = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32)\n",
    "    ignored_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n",
    "\n",
    "    index = np.arange(0, length, int(1/scale))\n",
    "    index_x, index_y = np.meshgrid(index, index)\n",
    "    \n",
    "    ignored_polys = []    #vertices with label '###'\n",
    "    polys = []            #vertices with word label\n",
    "\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        if labels[i] == 0: \n",
    "            ignored_polys.append(np.around(scale * vertice.reshape((4,2))).astype(np.int32))\n",
    "            continue\n",
    "\n",
    "        poly = np.around(scale * shrink_poly(vertice).reshape((4,2))).astype(np.int32) # scaled & shrinked\n",
    "        polys.append(poly)\n",
    "        temp_mask = np.zeros(score_map.shape[:-1], np.float32)    \n",
    "        cv2.fillPoly(temp_mask, [poly], 1)    #score_map 0/1\n",
    "\n",
    "        theta = find_min_rect_angle(vertice)\n",
    "        rotate_mat = get_rotate_mat(theta)\n",
    "\n",
    "        rotated_vertices = rotate_vertices(vertice, theta)\n",
    "        x_min, x_max, y_min, y_max = get_boundary(rotated_vertices)\n",
    "        rotated_x, rotated_y = rotate_all_pixels(rotate_mat, vertice[0], vertice[1], length)\n",
    "\n",
    "        d1 = rotated_y - y_min\n",
    "        d1[d1<0] = 0\n",
    "        d2 = y_max - rotated_y\n",
    "        d2[d2<0] = 0\n",
    "        d3 = rotated_x - x_min\n",
    "        d3[d3<0] = 0\n",
    "        d4 = x_max - rotated_x\n",
    "        d4[d4<0] = 0\n",
    "        geo_map[:,:,0] += d1[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,1] += d2[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,2] += d3[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,3] += d4[index_y, index_x] * temp_mask\n",
    "        geo_map[:,:,4] += theta * temp_mask\n",
    "\n",
    "#     cv2.fillPoly(ignored_map, ignored_polys, 1)\n",
    "    cv2.fillPoly(score_map, polys, 1)\n",
    "    return torch.Tensor(score_map).permute(2,0,1), torch.Tensor(geo_map).permute(2,0,1), torch.Tensor(ignored_map).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(data.Dataset):\n",
    "    def __init__(self, img_path, gt_path, scale=0.25, length=512):\n",
    "        super(custom_dataset, self).__init__()\n",
    "        self.img_files = [os.path.join(img_path, img_file) for img_file in sorted(os.listdir(img_path))]\n",
    "        self.gt_files  = [os.path.join(gt_path, gt_file) for gt_file in sorted(os.listdir(gt_path))]\n",
    "        self.scale = scale\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with open(self.gt_files[index], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        vertices, labels = extract_vertices(lines)\n",
    "\n",
    "        img = Image.open(self.img_files[index])\n",
    "        img, vertices = adjust_height(img, vertices) \n",
    "        img, vertices = rotate_img(img, vertices)\n",
    "        img, vertices = crop_img(img, vertices, labels, self.length) \n",
    "        transform = transforms.Compose([transforms.ColorJitter(0.5, 0.5, 0.5, 0.25), \\\n",
    "                                        transforms.ToTensor(), \\\n",
    "                                        transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "\n",
    "        score_map, geo_map, ignored_map = get_score_geo(img, vertices, labels, self.scale, self.length)\n",
    "        return transform(img), score_map, geo_map, ignored_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64])\n",
      "torch.Size([1, 5, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class extractor(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(extractor, self).__init__()\n",
    "        vgg16_bn = VGG(make_layers(cfg, batch_norm=True))\n",
    "        if pretrained:\n",
    "            vgg16_bn.load_state_dict(torch.load('./pths/vgg16_bn-6c64b313.pth'))\n",
    "        self.features = vgg16_bn.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        for m in self.features:\n",
    "            x = m(x)\n",
    "            if isinstance(m, nn.MaxPool2d):\n",
    "                out.append(x)\n",
    "        return out[1:]\n",
    "\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(merge, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(384, 64, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(192, 32, 1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.conv6 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(32)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.interpolate(x[3], scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[2]), 1)\n",
    "        y = self.relu1(self.bn1(self.conv1(y)))\n",
    "        y = self.relu2(self.bn2(self.conv2(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[1]), 1)\n",
    "        y = self.relu3(self.bn3(self.conv3(y)))\n",
    "        y = self.relu4(self.bn4(self.conv4(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[0]), 1)\n",
    "        y = self.relu5(self.bn5(self.conv5(y)))\n",
    "        y = self.relu6(self.bn6(self.conv6(y)))\n",
    "\n",
    "        y = self.relu7(self.bn7(self.conv7(y)))\n",
    "        return y\n",
    "\n",
    "class output(nn.Module):\n",
    "    def __init__(self, scope=512):\n",
    "        super(output, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(32, 1, 1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.conv2 = nn.Conv2d(32, 4, 1)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.conv3 = nn.Conv2d(32, 1, 1)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        self.scope = 512    #trick\n",
    "        \n",
    "        #weight initialazation\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = self.sigmoid1(self.conv1(x))\n",
    "        loc   = self.sigmoid2(self.conv2(x)) * self.scope\n",
    "        angle = (self.sigmoid3(self.conv3(x)) - 0.5) * math.pi\n",
    "        geo   = torch.cat((loc, angle), 1) \n",
    "        return score, geo\n",
    "\n",
    "\n",
    "class EAST(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EAST, self).__init__()\n",
    "        self.extractor = extractor(pretrained)\n",
    "        self.merge     = merge()\n",
    "        self.output    = output()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.merge(self.extractor(x)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = EAST()\n",
    "    x = torch.randn(1, 3, 256, 256)\n",
    "    score, geo = m(x)\n",
    "    print(score.shape)\n",
    "    print(geo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def get_dice_loss(gt_score, pred_score):\n",
    "    inter = torch.sum(gt_score * pred_score)\n",
    "    union = torch.sum(gt_score) + torch.sum(pred_score) + 1e-5\n",
    "    return 1. - (2 * inter / union)\n",
    "\n",
    "\n",
    "def get_geo_loss(gt_geo, pred_geo):\n",
    "    d1_gt, d2_gt, d3_gt, d4_gt, angle_gt = torch.split(gt_geo, 1, 1)\n",
    "    d1_pred, d2_pred, d3_pred, d4_pred, angle_pred = torch.split(pred_geo, 1, 1)\n",
    "    area_gt = (d1_gt + d2_gt) * (d3_gt + d4_gt)\n",
    "    area_pred = (d1_pred + d2_pred) * (d3_pred + d4_pred)\n",
    "    w_union = torch.min(d3_gt, d3_pred) + torch.min(d4_gt, d4_pred)\n",
    "    h_union = torch.min(d1_gt, d1_pred) + torch.min(d2_gt, d2_pred)\n",
    "    area_intersect = w_union * h_union\n",
    "    area_union = area_gt + area_pred - area_intersect\n",
    "    iou_loss_map = -torch.log((area_intersect + 1.0)/(area_union + 1.0))\n",
    "    angle_loss_map = 1 - torch.cos(angle_pred - angle_gt)\n",
    "    return iou_loss_map, angle_loss_map\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, weight_angle=10):\n",
    "        super(Loss, self).__init__()\n",
    "        self.weight_angle = weight_angle\n",
    "\n",
    "    def forward(self, gt_score, pred_score, gt_geo, pred_geo, ignored_map):\n",
    "        if torch.sum(gt_score) < 1:\n",
    "            return torch.sum(pred_score + pred_geo) * 0\n",
    "\n",
    "        classify_loss = get_dice_loss(gt_score, pred_score*(1-ignored_map))\n",
    "        iou_loss_map, angle_loss_map = get_geo_loss(gt_geo, pred_geo)\n",
    "\n",
    "        angle_loss = torch.sum(angle_loss_map*gt_score) / torch.sum(gt_score)\n",
    "        iou_loss = torch.sum(iou_loss_map*gt_score) / torch.sum(gt_score)\n",
    "        geo_loss = self.weight_angle * angle_loss + iou_loss\n",
    "        print('classify loss is {:.8f}, angle loss is {:.8f}, iou loss is {:.8f}'.format(classify_loss, angle_loss, iou_loss))\n",
    "        return geo_loss + classify_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify loss is 0.99409354, angle loss is 0.79824001, iou loss is 5.32217789\n",
      "Epoch is [1/1], mini-batch is [1/500], time consumption is 7.08751202, batch_loss is 14.29867172\n",
      "classify loss is 0.99803394, angle loss is 0.65214497, iou loss is 5.25536203\n",
      "Epoch is [1/1], mini-batch is [2/500], time consumption is 4.78621554, batch_loss is 12.77484512\n",
      "classify loss is 0.99045634, angle loss is 0.77830940, iou loss is 4.47777176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/luoxiaosheng/anaconda3/envs/fastai/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-da8416f40f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mepoch_iter\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0msave_interval\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpths_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-da8416f40f9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, interval)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "from dataset import custom_dataset\n",
    "from model import EAST\n",
    "from loss import Loss\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, interval):\n",
    "    file_num = len(os.listdir(train_img_path))\n",
    "    trainset = custom_dataset(train_img_path, train_gt_path)\n",
    "    train_loader = data.DataLoader(trainset, batch_size=batch_size, \\\n",
    "                                   shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "    criterion = Loss()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST()\n",
    "    data_parallel = False\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        data_parallel = True\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[epoch_iter//2], gamma=0.1)\n",
    "\n",
    "    for epoch in range(epoch_iter):\n",
    "        model.train()\n",
    "        scheduler.step()\n",
    "        epoch_loss = 0\n",
    "        epoch_time = time.time()\n",
    "        for i, (img, gt_score, gt_geo, ignored_map) in enumerate(train_loader):\n",
    "            start_time = time.time()\n",
    "            img, gt_score, gt_geo, ignored_map = img.to(device), gt_score.to(device), gt_geo.to(device), ignored_map.to(device)\n",
    "            pred_score, pred_geo = model(img)\n",
    "            loss = criterion(gt_score, pred_score, gt_geo, pred_geo, ignored_map)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print('Epoch is [{}/{}], mini-batch is [{}/{}], time consumption is {:.8f}, batch_loss is {:.8f}'.format(\\\n",
    "              epoch+1, epoch_iter, i+1, int(file_num/batch_size), time.time()-start_time, loss.item()))\n",
    "\n",
    "        print('epoch_loss is {:.8f}, epoch_time is {:.8f}'.format(epoch_loss/int(file_num/batch_size), time.time()-epoch_time))\n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        print('='*50)\n",
    "        if (epoch + 1) % interval == 0:\n",
    "            state_dict = model.module.state_dict() if data_parallel else model.state_dict()\n",
    "            torch.save(state_dict, os.path.join(pths_path, 'model_epoch_{}.pth'.format(epoch+1)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_img_path = os.path.abspath('../ICDAR_2015/train_img')\n",
    "    train_gt_path  = os.path.abspath('../ICDAR_2015/train_gt')\n",
    "    pths_path      = './pths'\n",
    "    batch_size     = 2 \n",
    "    lr             = 1e-3\n",
    "    num_workers    = 4\n",
    "    epoch_iter     = 1\n",
    "    save_interval  = 5\n",
    "    train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, save_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# locality_aware_nms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def intersection(g, p):\n",
    "    g = Polygon(g[:8].reshape((4, 2)))\n",
    "    p = Polygon(p[:8].reshape((4, 2)))\n",
    "    if not g.is_valid or not p.is_valid:\n",
    "        return 0\n",
    "    inter = Polygon(g).intersection(Polygon(p)).area\n",
    "    union = g.area + p.area - inter\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return inter/union\n",
    "\n",
    "\n",
    "def weighted_merge(g, p):\n",
    "    g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8])\n",
    "    g[8] = (g[8] + p[8])\n",
    "    return g\n",
    "\n",
    "\n",
    "def standard_nms(S, thres):\n",
    "    order = np.argsort(S[:, 8])[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n",
    "\n",
    "        inds = np.where(ovr <= thres)[0]\n",
    "        order = order[inds+1]\n",
    "\n",
    "    return S[keep]\n",
    "\n",
    "\n",
    "def nms_locality(polys, thres=0.3):\n",
    "    '''\n",
    "    locality aware nms of EAST\n",
    "    :param polys: a N*9 numpy array. first 8 coordinates, then prob\n",
    "    :return: boxes after nms\n",
    "    '''\n",
    "    S = []\n",
    "    p = None\n",
    "    for g in polys:\n",
    "        if p is not None and intersection(g, p) > thres:\n",
    "            p = weighted_merge(g, p)\n",
    "        else:\n",
    "            if p is not None:\n",
    "                S.append(p)\n",
    "            p = g\n",
    "    if p is not None:\n",
    "        S.append(p)\n",
    "\n",
    "    if len(S) == 0:\n",
    "        return np.array([])\n",
    "    return standard_nms(np.array(S), thres)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 343,350,448,135,474,143,369,359\n",
    "    print(Polygon(np.array([[343, 350], [448, 135],\n",
    "                            [474, 143], [369, 359]])).area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from model import EAST\n",
    "import os\n",
    "from dataset import get_rotate_mat\n",
    "import numpy as np\n",
    "import lanms\n",
    "\n",
    "\n",
    "def resize_img(img):\n",
    "    '''resize image to be divisible by 32\n",
    "    '''\n",
    "    w, h = img.size\n",
    "    resize_w = w\n",
    "    resize_h = h\n",
    "\n",
    "    resize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32\n",
    "    resize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32\n",
    "    img = img.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "    ratio_h = resize_h / h\n",
    "    ratio_w = resize_w / w\n",
    "\n",
    "    return img, ratio_h, ratio_w\n",
    "\n",
    "\n",
    "def load_pil(img):\n",
    "    '''convert PIL Image to torch.Tensor\n",
    "    '''\n",
    "    t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "    return t(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def is_valid_poly(res, score_shape, scale):\n",
    "    '''check if the poly in image scope\n",
    "    Input:\n",
    "        res        : restored poly in original image\n",
    "        score_shape: score map shape\n",
    "        scale      : feature map -> image\n",
    "    Output:\n",
    "        True if valid\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for i in range(res.shape[1]):\n",
    "        if res[0,i] < 0 or res[0,i] >= score_shape[1] * scale or \\\n",
    "           res[1,i] < 0 or res[1,i] >= score_shape[0] * scale:\n",
    "            cnt += 1\n",
    "    return True if cnt <= 1 else False\n",
    "\n",
    "\n",
    "def restore_polys(valid_pos, valid_geo, score_shape, scale=4):\n",
    "    '''restore polys from feature maps in given positions\n",
    "    Input:\n",
    "        valid_pos  : potential text positions <numpy.ndarray, (n,2)>\n",
    "        valid_geo  : geometry in valid_pos <numpy.ndarray, (5,n)>\n",
    "        score_shape: shape of score map\n",
    "        scale      : image / feature map\n",
    "    Output:\n",
    "        restored polys <numpy.ndarray, (n,8)>, index\n",
    "    '''\n",
    "    polys = []\n",
    "    index = []\n",
    "    valid_pos *= scale\n",
    "    d = valid_geo[:4, :] # 4 x N\n",
    "    angle = valid_geo[4, :] # N,\n",
    "\n",
    "    for i in range(valid_pos.shape[0]):\n",
    "        x = valid_pos[i, 0]\n",
    "        y = valid_pos[i, 1]\n",
    "        y_min = y - d[0, i]\n",
    "        y_max = y + d[1, i]\n",
    "        x_min = x - d[2, i]\n",
    "        x_max = x + d[3, i]\n",
    "        rotate_mat = get_rotate_mat(-angle[i])\n",
    "\n",
    "        temp_x = np.array([[x_min, x_max, x_max, x_min]]) - x\n",
    "        temp_y = np.array([[y_min, y_min, y_max, y_max]]) - y\n",
    "        coordidates = np.concatenate((temp_x, temp_y), axis=0)\n",
    "        res = np.dot(rotate_mat, coordidates)\n",
    "        res[0,:] += x\n",
    "        res[1,:] += y\n",
    "\n",
    "        if is_valid_poly(res, score_shape, scale):\n",
    "            index.append(i)\n",
    "            polys.append([res[0,0], res[1,0], res[0,1], res[1,1], res[0,2], res[1,2],res[0,3], res[1,3]])\n",
    "    return np.array(polys), index\n",
    "\n",
    "\n",
    "def get_boxes(score, geo, score_thresh=0.9, nms_thresh=0.2):\n",
    "    '''get boxes from feature map\n",
    "    Input:\n",
    "        score       : score map from model <numpy.ndarray, (1,row,col)>\n",
    "        geo         : geo map from model <numpy.ndarray, (5,row,col)>\n",
    "        score_thresh: threshold to segment score map\n",
    "        nms_thresh  : threshold in nms\n",
    "    Output:\n",
    "        boxes       : final polys <numpy.ndarray, (n,9)>\n",
    "    '''\n",
    "    score = score[0,:,:]\n",
    "    xy_text = np.argwhere(score > score_thresh) # n x 2, format is [r, c]\n",
    "    if xy_text.size == 0:\n",
    "        return None\n",
    "\n",
    "    xy_text = xy_text[np.argsort(xy_text[:, 0])]\n",
    "    valid_pos = xy_text[:, ::-1].copy() # n x 2, [x, y]\n",
    "    valid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]] # 5 x n\n",
    "    polys_restored, index = restore_polys(valid_pos, valid_geo, score.shape) \n",
    "    if polys_restored.size == 0:\n",
    "        return None\n",
    "\n",
    "    boxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\n",
    "    boxes[:, :8] = polys_restored\n",
    "    boxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\n",
    "    boxes = nms_locality(boxes.astype('float32'), nms_thresh)  #lanms.merge_quadrangle_n9\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def adjust_ratio(boxes, ratio_w, ratio_h):\n",
    "    '''refine boxes\n",
    "    Input:\n",
    "        boxes  : detected polys <numpy.ndarray, (n,9)>\n",
    "        ratio_w: ratio of width\n",
    "        ratio_h: ratio of height\n",
    "    Output:\n",
    "        refined boxes\n",
    "    '''\n",
    "    if boxes is None or boxes.size == 0:\n",
    "        return None\n",
    "    boxes[:,[0,2,4,6]] /= ratio_w\n",
    "    boxes[:,[1,3,5,7]] /= ratio_h\n",
    "    return np.around(boxes)\n",
    "\n",
    "\n",
    "def detect(img, model, device):\n",
    "    '''detect text regions of img using model\n",
    "    Input:\n",
    "        img   : PIL Image\n",
    "        model : detection model\n",
    "        device: gpu if gpu is available\n",
    "    Output:\n",
    "        detected polys\n",
    "    '''\n",
    "    img, ratio_h, ratio_w = resize_img(img)\n",
    "    with torch.no_grad():\n",
    "        score, geo = model(load_pil(img).to(device))\n",
    "    boxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\n",
    "    return adjust_ratio(boxes, ratio_w, ratio_h)\n",
    "\n",
    "\n",
    "def plot_boxes(img, boxes):\n",
    "    '''plot boxes on image\n",
    "    '''\n",
    "    if boxes is None:\n",
    "        return img\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes:\n",
    "        draw.polygon([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]], outline=(0,255,0))\n",
    "    return img\n",
    "\n",
    "\n",
    "def detect_dataset(model, device, test_img_path, submit_path):\n",
    "    '''detection on whole dataset, save .txt results in submit_path\n",
    "    Input:\n",
    "        model        : detection model\n",
    "        device       : gpu if gpu is available\n",
    "        test_img_path: dataset path\n",
    "        submit_path  : submit result for evaluation\n",
    "    '''\n",
    "    img_files = os.listdir(test_img_path)\n",
    "    img_files = sorted([os.path.join(test_img_path, img_file) for img_file in img_files])\n",
    "\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        print('evaluating {} image'.format(i), end='\\r')\n",
    "        boxes = detect(Image.open(img_file), model, device)\n",
    "        seq = []\n",
    "        if boxes is not None:\n",
    "            seq.extend([','.join([str(int(b)) for b in box[:-1]]) + '\\n' for box in boxes])\n",
    "        with open(os.path.join(submit_path, 'res_' + os.path.basename(img_file).replace('.jpg','.txt')), 'w') as f:\n",
    "            f.writelines(seq)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img_path    = '../ICDAR_2015/test_img/img_38.jpg'\n",
    "    model_path  = './pths/east_vgg16.pth'\n",
    "    res_img     = './res.bmp'\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EAST().to(device)\n",
    "    model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "    model.eval()\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    boxes = detect(img, model, device)\n",
    "    plot_img = plot_boxes(img, boxes)\n",
    "    plot_img.save(res_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=0.25\n",
    "length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_txt, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "vertices, labels = extract_vertices(lines)\n",
    "\n",
    "img = Image.open(path_img)\n",
    "# img, vertices = adjust_height(img, vertices)\n",
    "# img, vertices = rotate_img(img, vertices)\n",
    "# img, vertices = crop_img(img, vertices, labels, length)\n",
    "\n",
    "transform = transforms.Compose([transforms.ColorJitter(0.5, 0.5, 0.5, 0.25), \\\n",
    "                                transforms.ToTensor(), \\\n",
    "                                transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))])\n",
    "\n",
    "score_map, geo_map, ignored_map = get_score_geo(img, vertices, labels, scale, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone() # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0) # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        plt.pause(0.001) # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(score_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ignored_map.numpy():\n",
    "    for j in i:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(ignored_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(geo_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (fastai)",
   "language": "python",
   "name": "python3_fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
